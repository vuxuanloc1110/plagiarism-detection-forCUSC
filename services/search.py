# search.py

# ‚úÖ Import c·∫•u h√¨nh chung
from config import BRAVE_API_KEY, BRAVE_SEARCH_URL, INDEX_NAME
import asyncio
import aiohttp
# ‚úÖ Import model v√† device
from models.model_loader import load_model
model, device = load_model()

# ‚úÖ Import h√†m x·ª≠ l√Ω docx/html/highlight
from services.docx_utils import (
    convert_text_to_html,
    read_text_file,
    split_into_sentences,
    split_into_sentences_txt
)

# ‚úÖ Import ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
from services.preprocess import (
    extract_keywords,
    select_top_tfidf_sentences_percentile,
    highlight_text,
)

# ‚úÖ Import ch·ª©c nƒÉng t√¨m ki·∫øm Google
from services.google_search import (
    batch_google_search
)

# ‚úÖ Th∆∞ vi·ªán h·ªá th·ªëng v√† NLP
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
import torch
import time
import re
from sentence_transformers import util
import numpy as np
from elasticsearch import Elasticsearch

# ‚úÖ Kh·ªüi t·∫°o Elasticsearch (ho·∫∑c import es t·ª´ n∆°i kh√°c)
es = Elasticsearch("http://localhost:9200")
# Load once at the top of the file
model, device = load_model()
def remove_html_tags(text):
    """Remove HTML tags from text"""
    if not text:
        return text
    # Remove HTML tags using regex
    clean_text = re.sub(r'<[^>]+>', '', text)
    # Also decode common HTML entities
    clean_text = clean_text.replace('&amp;', '&')
    clean_text = clean_text.replace('&lt;', '<')
    clean_text = clean_text.replace('&gt;', '>')
    clean_text = clean_text.replace('&quot;', '"')
    clean_text = clean_text.replace('&#39;', "'")
    return clean_text.strip()

async def batch_brave_search(sentences, max_workers=1):
    """Batch search sentences using Brave Search API"""
    
    async def search_single_sentence(session, sentence):
        """Search a single sentence using Brave Search API"""
        try:
            # Add 1 second delay before each request
            await asyncio.sleep(1)
            
            headers = {
                'Accept': 'application/json',
                'Accept-Encoding': 'gzip',
                'X-Subscription-Token': BRAVE_API_KEY
            }
            
            params = {
                'q': f'"{sentence}"',  # Use quotes for exact phrase search
                'count': 1,  # Number of results to return
                'offset': 0,
                'search_lang': 'vi',
                'safesearch': 'moderate', 
            }
            
            async with session.get(BRAVE_SEARCH_URL, headers=headers, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # Extract the best result
                    if data.get('web', {}).get('results'):
                        best_result = data['web']['results'][0]
                        snippet = remove_html_tags(best_result.get('description', ''))
                        title = remove_html_tags(best_result.get('title', ''))
                        return {
                            "result": "Found",
                            "snippet": snippet,
                            "url": best_result.get('url', ''),
                            "title": title
                        }
                    else:
                        return {"result": "Not Found", "snippet": "", "url": ""}
                else:
                    print(f"Brave Search API error: {response.status}")
                    return {"result": "Not Found", "snippet": "", "url": ""}
                    
        except Exception as e:
            print(f"Error searching sentence: {e}")
            return {"result": "Not Found", "snippet": "", "url": ""}
    
    # Create semaphore to limit concurrent requests
    semaphore = asyncio.Semaphore(max_workers)
    
    async def search_with_semaphore(session, sentence):
        async with semaphore:
            return await search_single_sentence(session, sentence)
    
    # Execute all searches concurrently
    async with aiohttp.ClientSession() as session:
        tasks = [search_with_semaphore(session, sentence) for sentence in sentences]
        results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Handle any exceptions that occurred
    processed_results = []
    for result in results:
        if isinstance(result, Exception):
            print(f"Search exception: {result}")
            processed_results.append({"result": "Not Found", "snippet": "", "url": ""})
        else:
            processed_results.append(result)
    
    return processed_results

async def search_similar_sentences(file_path, top_k=3, threshold=0.5, mode="both"):
    if mode not in ["elastic", "brave", "both"]:
        raise ValueError(f"Ch·∫ø ƒë·ªô kh√¥ng h·ª£p l·ªá: {mode}")

    print(f"üîß ƒêang ch·∫°y v·ªõi ch·∫ø ƒë·ªô t√¨m ki·∫øm: {mode}")  # üëâ ƒê·∫∑t ·ªü ƒë√¢y

    start_time = time.time()
    doc_content, sentence_map = convert_text_to_html(file_path)
    input_sentences = list(sentence_map.values())
    
    sentence_embeddings = model.encode(input_sentences, convert_to_tensor=True, device=device, batch_size=256)
    best_matches = defaultdict(lambda: {"sentence_similarity": -1})
    seen_sentences = set()
    search_results = {}

    if mode in ["elastic", "both"]:
        # H√†m t√¨m ki·∫øm m·ªôt c√¢u trong Elasticsearch
        def search_query(args):
            idx, sentence, embedding = args
            if sentence in seen_sentences:
                return None
            
            seen_sentences.add(sentence)
            query = {
                "size": top_k,
                "knn": {
                    "field": "embedding",
                    "query_vector": embedding.cpu().numpy().tolist(),
                    "k": top_k,
                    "num_candidates": 50,
                }
            }
            response = es.search(index=INDEX_NAME, body=query)
            return (sentence, {"idx": idx, "response": response, "embedding": embedding})

        # T√¨m ki·∫øm trong Elasticsearch song song
        search_start = time.time()
        with ThreadPoolExecutor(max_workers=min(16, len(input_sentences))) as executor:
            tasks = [(idx, input_sentences[idx - 1], sentence_embeddings[idx - 1]) 
                     for idx in range(1, len(input_sentences) + 1)]
            search_results_list = list(executor.map(search_query, tasks))
        
        # L·ªçc b·ªè k·∫øt qu·∫£ None
        search_results = {sentence: data for result in search_results_list if result for sentence, data in [result]}
        search_end = time.time()
        print(f"üîç Ho√†n th√†nh t√¨m ki·∫øm trong Elasticsearch ({search_end - search_start:.2f} gi√¢y)")

        # X·ª≠ l√Ω k·∫øt qu·∫£ Elasticsearch
        all_keywords = {sentence: set(extract_keywords(sentence)) for sentence in search_results}
        unique_keywords = set().union(*all_keywords.values())
        keyword_embeddings_global = {
            kw: model.encode(kw, convert_to_tensor=True, device=device) for kw in unique_keywords
        }

        for sentence, data in search_results.items():
            idx, response, sentence_embedding = data.values()
            keywords = all_keywords[sentence]

            for res in response["hits"]["hits"]:
                matched_sentence = res["_source"]["sentence"]
                matched_embedding = torch.tensor(res["_source"]["embedding"], device=device)

                common_keywords = keywords & set(extract_keywords(matched_sentence))
                if len(common_keywords) >= 3:
                    sentence_similarity = round(util.cos_sim(sentence_embedding, matched_embedding).item(), 4)

                    if sentence_similarity >= threshold:
                        if common_keywords:
                            kw_embeddings = torch.stack([keyword_embeddings_global[kw] for kw in common_keywords])
                            keyword_similarities = util.cos_sim(kw_embeddings, matched_embedding).mean().item()
                        else:
                            keyword_similarities = 0.0

                        if sentence_similarity > best_matches[sentence]["sentence_similarity"]:
                            best_matches[sentence] = {
                                "matched_sentence": matched_sentence,
                                "source": "Elasticsearch",
                                "filename": res["_source"].get("filename", ""),
                                "sentence_similarity": sentence_similarity,
                                "keywords_similarity": round(keyword_similarities, 4),
                                "sentence_id": idx,
                                "keywords": list(common_keywords),
                            }

    # T√¨m c√°c c√¢u ch∆∞a match ƒë·ªß t·ªët ƒë·ªÉ t√¨m v·ªõi Brave
    unmatched_sentences = []
    if mode in ["both", "brave"]:
        unmatched_sentences = [s for s in input_sentences if best_matches[s]["sentence_similarity"] < threshold]
        unmatched_sentences = [s for s in unmatched_sentences if len(s.split()) >= 5]

        if unmatched_sentences:
            top_indices = select_top_tfidf_sentences_percentile(unmatched_sentences, percentile=100)
            top_sentences = [unmatched_sentences[i] for i in top_indices]

            brave_start = time.time()
            brave_results = await batch_brave_search(top_sentences)
            brave_end = time.time()
            print(f"üîé Ho√†n th√†nh t√¨m ki·∫øm trong Brave Search ({brave_end - brave_start:.2f} gi√¢y)")

            for idx, (sentence, brave_result) in enumerate(zip(top_sentences, brave_results), start=1):
                if brave_result.get("result") == "Not Found":
                    continue

                snippet = brave_result.get("snippet", "").strip()
                brave_url = brave_result.get("url", "")
                if not snippet:
                    continue

                snippet_sentences = split_into_sentences(snippet)
                sentence_embedding = sentence_embeddings[input_sentences.index(sentence)]

                best_snippet = ""
                best_similarity = 0.0
                for snip_sent in snippet_sentences:
                    if not snip_sent:
                        continue
                    snip_embedding = model.encode(snip_sent, convert_to_tensor=True, device=device)
                    similarity = util.cos_sim(sentence_embedding, snip_embedding).item()
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_snippet = snip_sent

                if best_similarity >= 0.5:
                    best_matches[sentence] = {
                        "matched_sentence": best_snippet,
                        "source": "Brave Search",
                        "filename": brave_url,
                        "url": brave_url,
                        "sentence_similarity": round(best_similarity, 4),
                        "sentence_id": idx,
                        "keywords_similarity": 0,
                        "keywords": []
                    }

    print("‚úÖ Ho√†n th√†nh to√†n b·ªô qu√° tr√¨nh t√¨m ki·∫øm")

    results = sorted([
        {
            "original_sentence": orig,
            "matched_sentence": data.get("matched_sentence", ""),
            "source": data.get("source", ""),
            "highlighted_original_sentence": orig,
            "highlighted_matched_sentence": data.get("matched_sentence", ""), 
            "filename": data.get("filename", ""),
            "url": data.get("url", ""),
            "sentence_similarity": data.get("sentence_similarity", -1),
            "keywords_similarity": data.get("keywords_similarity", 0),
            "sentence_id": data.get("sentence_id", -1)
        }
        for orig, data in best_matches.items()
        if data.get("matched_sentence")
    ], key=lambda x: x["sentence_similarity"], reverse=True)

    doc_content = highlight_text(doc_content, results, sentence_map)
    print(f"‚è≥ T·ªïng th·ªùi gian th·ª±c thi: {time.time() - start_time:.2f} gi√¢y")

    return doc_content, results

import re
import os
def split_into_sentences_txt(text_or_path):

    # B·∫£o v·ªá d·∫•u ch·∫•m ƒë·ª©ng tr∆∞·ªõc chu·ªói c√≥ ch·ª©a '.docx' ho·∫∑c c√°c m√£ UUID d·∫°ng <...docxdocx>
    protected_text = re.sub(r'\.(?=\s*<[^>]*?docx[^>]*?>)', '<DOT_DOCX>', text_or_path.strip())

    # T√°ch c√¢u nh∆∞ b√¨nh th∆∞·ªùng
    sentence_endings = r'(?<=[.!?:])\s+'
    sentences = re.split(sentence_endings, protected_text)

    # Kh√¥i ph·ª•c l·∫°i '.docx'
    sentences = [s.replace('<DOT_DOCX>', '.').strip() for s in sentences if s.strip()]
    return sentences


async def search_similar_sentences_txt(file_path):
    """T√¨m ki·∫øm c√¢u tr√πng l·∫∑p cho file .txt"""
    start_time = time.time()
    
    # ƒê·ªçc n·ªôi dung file .txt
    text_content = read_text_file(file_path)
    if not text_content:
        return "", []

    # Chia n·ªôi dung th√†nh c√°c c√¢u
    input_sentences = split_into_sentences(text_content)
    print(input_sentences)
    # T·∫°o sentence_map v·ªõi ID ƒë∆°n gi·∫£n
    sentence_map = {f"sentence-{i+1}": sentence for i, sentence in enumerate(input_sentences)}
    
    # T·∫°o HTML ƒë∆°n gi·∫£n cho n·ªôi dung
    html_content = []
    for idx, sentence in enumerate(input_sentences, 1):
        html_content.append(f'<span id="sentence-{idx}">{sentence}</span>')
    doc_content = "<br><br>".join(html_content)

    # M√£ h√≥a c√°c c√¢u
    sentence_embeddings = model.encode(input_sentences, convert_to_tensor=True, device=device, batch_size=256)
    
    # T√°i s·ª≠ d·ª•ng logic t√¨m ki·∫øm t·ª´ search_similar_sentences
    best_matches = defaultdict(lambda: {"sentence_similarity": -1})
    seen_sentences = set()
    search_results = {}

    def search_query(args):
        idx, sentence, embedding = args
        if sentence in seen_sentences:
            return None
        seen_sentences.add(sentence)
        query = {
            "size": 3,
            "knn": {
                "field": "embedding",
                "query_vector": embedding.cpu().numpy().tolist(),
                "k": 3,
                "num_candidates": 50,
            }
        }
        response = es.search(index=INDEX_NAME, body=query)
        return (sentence, {"idx": idx, "response": response, "embedding": embedding})
    search_start = time.time()
    # T√¨m ki·∫øm trong Elasticsearch
    with ThreadPoolExecutor(max_workers=min(16, len(input_sentences))) as executor:
        tasks = [(idx, sentence, sentence_embeddings[idx - 1]) 
                 for idx, sentence in enumerate(input_sentences, 1)]
        search_results_list = list(executor.map(search_query, tasks))
    
    search_results = {sentence: data for result in search_results_list if result for sentence, data in [result]}


    search_end = time.time()
    print(f"üîç Ho√†n th√†nh t√¨m ki·∫øm trong Elasticsearch ({search_end - search_start:.2f} gi√¢y)")
    # X·ª≠ l√Ω k·∫øt qu·∫£ Elasticsearch
    all_keywords = {sentence: set(extract_keywords(sentence)) for sentence in search_results}
    unique_keywords = set().union(*all_keywords.values())
    keyword_embeddings_global = {kw: model.encode(kw, convert_to_tensor=True, device=device) 
                                for kw in unique_keywords}

    for sentence, data in search_results.items():
        idx, response, sentence_embedding = data.values()
        keywords = all_keywords[sentence]
        
        for res in response["hits"]["hits"]:
            matched_sentence = res["_source"]["sentence"]
            matched_embedding = torch.tensor(res["_source"]["embedding"], device=device)
            
            common_keywords = keywords & set(extract_keywords(matched_sentence))
            if len(common_keywords) >= 3:
                sentence_similarity = round(util.cos_sim(sentence_embedding, matched_embedding).item(), 4)

                if sentence_similarity >= 0.5:
                    kw_embeddings = torch.stack([keyword_embeddings_global[kw] for kw in common_keywords])
                    keyword_similarities = util.cos_sim(kw_embeddings, matched_embedding).mean().item()
                    
                    if sentence_similarity > best_matches[sentence]["sentence_similarity"]:
                        best_matches[sentence] = {
                            "matched_sentence": matched_sentence,
                            "source": "Elasticsearch",
                            "filename": res["_source"].get("filename", ""),
                            "sentence_similarity": sentence_similarity,
                            "keywords_similarity": round(keyword_similarities, 4),
                            "sentence_id": idx,
                            "keywords": list(common_keywords),
                        }

    # T√¨m ki·∫øm Google cho c√°c c√¢u kh√¥ng kh·ªõp
    unmatched_sentences = [s for s in input_sentences if best_matches[s]["sentence_similarity"] < 0.5]
    unmatched_sentences = [s for s in unmatched_sentences if len(s.split()) >= 5]
    if unmatched_sentences:
        top_indices = select_top_tfidf_sentences_percentile(unmatched_sentences, percentile=50)
        top_sentences = [unmatched_sentences[i] for i in top_indices]

        google_results = asyncio.run(batch_google_search(top_sentences))
        for idx, (sentence, google_result) in enumerate(zip(top_sentences, google_results), start=1):
            if google_result.get("result") == "Not Found":
                continue

            snippet = google_result.get("snippet", "").strip()
            google_url = google_result.get("url", "")
            if not snippet:
                continue

            snippet_sentences = split_into_sentences(snippet)
            sentence_embedding = sentence_embeddings[input_sentences.index(sentence)]
            best_snippet = ""
            best_similarity = 0.0

            for snip_sent in snippet_sentences:
                if not snip_sent:
                    continue
                snip_embedding = model.encode(snip_sent, convert_to_tensor=True, device=device)
                similarity = util.cos_sim(sentence_embedding, snip_embedding).item()
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_snippet = snip_sent

            if best_similarity >= 0.85:
                best_matches[sentence] = {
                    "matched_sentence": best_snippet,
                    "source": "Google Search",
                    "filename": google_url,
                    "url": google_url,
                    "sentence_similarity": round(best_similarity, 4),
                    "sentence_id": idx,
                    "keywords_similarity": 0,
                    "keywords": []
                }

    # T·∫°o k·∫øt qu·∫£
    results = sorted([
        {
            "original_sentence": orig,
            "matched_sentence": data.get("matched_sentence", ""),
            "source": data.get("source", ""),
            "highlighted_original_sentence": orig,
            "highlighted_matched_sentence": data.get("matched_sentence", ""), 
            "filename": data.get("filename", ""),
            "url": data.get("url", ""),
            "sentence_similarity": data.get("sentence_similarity", -1),
            "keywords_similarity": data.get("keywords_similarity", 0),
            "sentence_id": data.get("sentence_id", -1)
        }
        for orig, data in best_matches.items()
        if data.get("matched_sentence")
    ], key=lambda x: x["sentence_similarity"], reverse=True)

    # Highlight n·ªôi dung
    doc_content = highlight_text(doc_content, results, sentence_map)
    
    print(f"‚è≥ T·ªïng th·ªùi gian th·ª±c thi: {time.time() - start_time:.2f} gi√¢y")
    return doc_content, results
from werkzeug.utils import secure_filename