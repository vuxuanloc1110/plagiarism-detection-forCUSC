import json

def load_urls_from_json(file_path):
    """ƒê·ªçc danh s√°ch URL t·ª´ file JSON."""
    with open(file_path, 'r', encoding='utf-8') as f:
        urls = json.load(f)
    return urls
def modify_urls(urls):
    """Th√™m 'text.' v√†o tr∆∞·ªõc domain c·ªßa URL, d√π l√† 123docz.com hay 123docz.net."""
    modified_urls = []
    for url in urls:
        modified_url = re.sub(r"https://(www\.)?(123docz\.(com|net))", "https://text.123docz.net", url)
        modified_urls.append(modified_url)
    return modified_urls
import os
import time
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# Kh·ªüi t·∫°o Selenium WebDriver (c·∫•u h√¨nh ph√π h·ª£p v·ªõi h·ªá th·ªëng c·ªßa b·∫°n)
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # Ch·∫°y ch·∫ø ƒë·ªô ·∫©n ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n
driver = webdriver.Chrome(options=options)

DOWNLOAD_FOLDER = "downloaded_pages"
TEXT_FOLDER = "extracted_texts"
os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)
os.makedirs(TEXT_FOLDER, exist_ok=True)

def download_html(url):
    """T·∫£i trang HTML v√† l∆∞u v√†o th∆∞ m·ª•c."""
    driver.get(url)
    try:
        WebDriverWait(driver, 40).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        print(f"‚úÖ T·∫£i xong: {url}")
    except:
        print(f"‚ùå L·ªói t·∫£i trang: {url}")
        return None

    time.sleep(3)  # Ch·ªù ƒë·ªÉ trang t·∫£i xong
    page_source = driver.page_source

    # L∆∞u HTML v√†o file
    file_name = re.sub(r'[\\/*?:"<>|]', '_', url.split('/')[-1]) + ".html"
    file_path = os.path.join(DOWNLOAD_FOLDER, file_name)
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(page_source)

    return page_source, file_name

def extract_text_from_html(html_content):
    """Tr√≠ch xu·∫•t n·ªôi dung t·ª´ th·∫ª ch·ª©a b√†i vi·∫øt ch√≠nh."""
    soup = BeautifulSoup(html_content, "html.parser")
    content_div = soup.find("div", class_="vf_view_pc md:col-span-7 col-span-12 pb-16 relative")
    
    if not content_div:
        print("‚ùå Kh√¥ng t√¨m th·∫•y n·ªôi dung ch√≠nh!")
        return ""

    return content_div.get_text(separator="\n", strip=True)

def save_text(text, filename):
    """L∆∞u vƒÉn b·∫£n v√†o file."""
    text_path = os.path.join(TEXT_FOLDER, filename + ".txt")
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"‚úÖ ƒê√£ l∆∞u vƒÉn b·∫£n t·∫°i: {text_path}")
    # ƒê·ªçc danh s√°ch URL t·ª´ file JSON
collected_urls = load_urls_from_json("collected_urls.json")

# S·ª≠a URL ƒë·ªÉ th√™m 'text.'
modified_urls = modify_urls(collected_urls)

# T·∫£i t·ª´ng URL v√† x·ª≠ l√Ω n·ªôi dung
for url in modified_urls:
    print(f"üì• ƒêang t·∫£i: {url}")
    html_content, file_name = download_html(url)

    if html_content:
        text_content = extract_text_from_html(html_content)
        save_text(text_content, file_name)

# ƒê√≥ng tr√¨nh duy·ªát
driver.quit()
print("‚úÖ Ho√†n t·∫•t t·∫£i v√† x·ª≠ l√Ω n·ªôi dung!")
