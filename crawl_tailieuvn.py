import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy import signals
import json
import os

# Biáº¿n toÃ n cá»¥c Ä‘á»ƒ lÆ°u danh sÃ¡ch URL
urls_collected = []

# Load danh sÃ¡ch URL Ä‘Ã£ crawl trÆ°á»›c Ä‘Ã³ (náº¿u cÃ³)
FILE_NAME = "tailieu_urls.json"

if os.path.exists(FILE_NAME):
    with open(FILE_NAME, "r", encoding="utf-8") as f:
        try:
            urls_collected = json.load(f)
            print(f"ğŸ”„ ÄÃ£ táº£i {len(urls_collected)} URL tá»« file {FILE_NAME}, tiáº¿p tá»¥c crawl...")
        except json.JSONDecodeError:
            print(f"âš ï¸ File {FILE_NAME} bá»‹ lá»—i, báº¯t Ä‘áº§u crawl tá»« Ä‘áº§u.")
            urls_collected = []

class TailieuSpider(scrapy.Spider):
    name = 'tailieu_spider'
    start_urls = ['https://tailieu.vn']
    
    custom_settings = {
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'DOWNLOAD_DELAY': 5,
    }
    
    def parse(self, response):
        # TÃ¬m vÃ  theo dÃµi táº¥t cáº£ cÃ¡c liÃªn káº¿t cÃ³ chá»©a /doc/
        for link in response.css('a::attr(href)').getall():
            if link.startswith('/'):
                link = response.urljoin(link)
                
            # Chá»‰ quan tÃ¢m Ä‘áº¿n cÃ¡c URL chá»©a /doc/
            if "/doc/" in link:
                if link not in urls_collected:
                    urls_collected.append(link)  # LÆ°u vÃ o danh sÃ¡ch trÃ¡nh trÃ¹ng láº·p
                    yield {'URL': link}
                    
                    # Chá»‰ crawl cÃ¡c trang liÃªn quan Ä‘áº¿n tÃ i liá»‡u
                    yield scrapy.Request(link, callback=self.parse)

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(TailieuSpider, cls).from_crawler(crawler, *args, **kwargs)
        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)
        return spider

    def spider_closed(self, spider):
        """HÃ m nÃ y sáº½ Ä‘Æ°á»£c gá»i khi Spider káº¿t thÃºc hoáº·c bá»‹ ngáº¯t (Ctrl + C)"""
        print(f"\nğŸ›‘ Spider Ä‘Ã£ dá»«ng! Tá»•ng sá»‘ URL Ä‘Ã£ thu tháº­p: {len(urls_collected)}")

        # LÆ°u tiáº¿p tá»¥c danh sÃ¡ch URL vÃ o file JSON
        with open(FILE_NAME, "w", encoding="utf-8") as f:
            json.dump(urls_collected, f, indent=4, ensure_ascii=False)
        
        print(f"\nâœ… Danh sÃ¡ch URL Ä‘Ã£ lÆ°u vÃ o file `{FILE_NAME}`, sáº½ tiáº¿p tá»¥c tá»« Ä‘Ã³ trong láº§n sau.")

# Cháº¡y Scrapy
if __name__ == "__main__":
    process = CrawlerProcess()
    process.crawl(TailieuSpider)
    process.start()